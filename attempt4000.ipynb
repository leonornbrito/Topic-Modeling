{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string, unicodedata\n",
    "import nltk\n",
    "import contractions\n",
    "import inflect\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def remove_between_square_brackets(text):\n",
    "    text = re.sub('_',' ',text)\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = fetch_20newsgroups(subset='all',remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = denoise_text(corpus.data[17655])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_contractions(text):\n",
    "    \"\"\"Replace contractions in string of text\"\"\"\n",
    "    return contractions.fix(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emails(text):\n",
    "    return re.sub('\\S*@\\S*\\s?', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers(text):\n",
    "    return re.sub(r'\\d+', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = replace_contractions(sample)\n",
    "sample = remove_emails(sample)\n",
    "sample = remove_numbers(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Not boring is right!!! It is more likely to because me to have a stroke!!\n",
      "I had to listen to game two on the Blackhawk station and I do not think I\n",
      "could take another call of \"And JOSEPH makes a SPEcTACular saaaaaave\".\n",
      "Now I am a Blues fan, but I do not want them to play \"like\" they played\n",
      "in game two. Sure the result was fine, but the Blues match up very well\n",
      "with the Hawks so they really do not need to treat them like the  Flames.\n",
      "We do not need a \"Monday Night Miracle\" to have a chance to beat Chicago.\n",
      "The Blues can do it on talent. At least over the hawks.\n",
      "\n",
      "Rich h.--- Go BLUES!!\n"
     ]
    }
   ],
   "source": [
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = nltk.word_tokenize(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "# def replace_numbers(words):\n",
    "#     \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "#     p = inflect.engine()\n",
    "#     new_words = []\n",
    "#     for word in words:\n",
    "#         if word.isdigit():\n",
    "#             new_word = p.number_to_words(word)\n",
    "#             new_words.append(new_word)\n",
    "#         else:\n",
    "#             new_words.append(word)\n",
    "#     return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    #words = replace_numbers(words)\n",
    "    words = remove_stopwords(words)\n",
    "    words = [word for word in words if len(word)>2]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['boring', 'right', 'likely', 'stroke', 'listen', 'game', 'two', 'blackhawk', 'station', 'think', 'could', 'take', 'another', 'call', 'joseph', 'makes', 'spectacular', 'saaaaaave', 'blues', 'fan', 'want', 'play', 'like', 'played', 'game', 'two', 'sure', 'result', 'fine', 'blues', 'match', 'well', 'hawks', 'really', 'need', 'treat', 'like', 'flames', 'need', 'monday', 'night', 'miracle', 'chance', 'beat', 'chicago', 'blues', 'talent', 'least', 'hawks', 'rich', 'blues']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lpott\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "words = normalize(words)\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\Stemmed:\n",
      " ['bor', 'right', 'lik', 'stroke', 'list', 'gam', 'two', 'blackhawk', 'stat', 'think', 'could', 'tak', 'anoth', 'cal', 'joseph', 'mak', 'spectacul', 'saaaaaav', 'blu', 'fan', 'want', 'play', 'lik', 'play', 'gam', 'two', 'sur', 'result', 'fin', 'blu', 'match', 'wel', 'hawk', 'real', 'nee', 'tre', 'lik', 'flam', 'nee', 'monday', 'night', 'mirac', 'chant', 'beat', 'chicago', 'blu', 'tal', 'least', 'hawk', 'rich', 'blu']\n"
     ]
    }
   ],
   "source": [
    "def stem_and_lemmatize(words):\n",
    "    stems = stem_words(words)\n",
    "    #lemmas = lemmatize_verbs(words)\n",
    "    return stems\n",
    "\n",
    "stems = stem_and_lemmatize(words)\n",
    "\n",
    "#print('Stemmed:\\n', stems)\n",
    "print('\\Stemmed:\\n', stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(doc):\n",
    "    return stem_and_lemmatize(normalize(nltk.word_tokenize(remove_numbers(remove_emails(replace_contractions(denoise_text(doc)))))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "counter = CountVectorizer(tokenizer=preprocessor,min_df=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lpott\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:272: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "C:\\Users\\lpott\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:272: UserWarning: \"b'...'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n"
     ]
    }
   ],
   "source": [
    "countvecs = counter.fit_transform(corpus.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": \n",
      ": I am considering buying a 1993 Chevy or GMC 4x4 full-size pickup with\n",
      ": the extended cab.  Any opinions about these vehicles?  Have there been\n",
      ": any significant problems?\n",
      ": \n",
      ": -- \n",
      ": Dick Grady           Salem, NH,  USA            grady@world.std.com\n",
      ": So many newsgroups, so little time!\n",
      "\n",
      "\n",
      "I bought a brand new 1992 Chevrolet K2500 HD 4x4 extended cab last\n",
      "May.  It has had many, many problems.  See my earler post that describes\n",
      "the situation.  I went to BBB arbitration, and they ruled that Chevrolet\n",
      "must buy it back from me.  If you do get one, stay away from the 5 speed\n",
      "manual with the deep low first gear.  They have put three of them in my\n",
      "truck so far.  After about 1,500 miles, overdrive either starts\n",
      "rattling or hissing loudly.  There is no way to fix them.  Chevrolet \n",
      "says that the noise is \"a characteristic of the transmission.\"\n",
      "\n",
      "Also, if you are planning to use your truck to tow, the\n",
      "gear ratios in that tranny suck.  On a steep hill, you get up to about\n",
      "55 MPH in second gear at 4,000 RPM (yellow line).  If you shift to third,\n",
      "the RPM drop to only 2,500, and you begin to loose speed.  I should\n",
      "point out that the 350 V8 they put in the HD (8600 GVW) trucks is a\n",
      "detuned motor compared to the one they put in the light duty ones.  They\n",
      "dropped the compression ratio, supposedly for \"engine longevity\"\n",
      "reasons.  So the light duty 350 may pull better than my truck does.\n",
      "Other things that have gone wrong include the ventilation fan (3 times \n",
      "so far), paint (had specs of rust embedded in the paint from being\n",
      "shipped by rail with no covering), and suspension parts (link between\n",
      "stabilizer and control arm fell off).\n",
      "\n",
      "Any company can make a bad individual car, Chevrolet included.  What\n",
      "really bothered me was the way they reacted.  They made no attempt\n",
      "to deal with me except to tell me to take it back to the dealer for \n",
      "them to attempt to fix it one more time.  So I bought a brand new\n",
      "Ford F250 HD Super Cab with a 460 and an automatic.  I will never\n",
      "buy another Chevrolet.\n"
     ]
    }
   ],
   "source": [
    "print(corpus.data[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['also', 'anoth', 'arbit', 'arm', 'attempt', 'autom', 'away',\n",
       "        'back', 'bad', 'begin', 'bet', 'both', 'bought', 'brand', 'buy',\n",
       "        'cab', 'car', 'charact', 'chevrolet', 'chevy', 'comp', 'company',\n",
       "        'compress', 'consid', 'control', 'cov', 'deal', 'deep', 'describ',\n",
       "        'dick', 'drop', 'duty', 'earl', 'eith', 'embed', 'engin', 'exceiv',\n",
       "        'extend', 'fan', 'far', 'fel', 'first', 'fix', 'ford', 'fulls',\n",
       "        'gear', 'get', 'gmc', 'gon', 'grady', 'hil', 'hiss', 'includ',\n",
       "        'individ', 'last', 'light', 'lin', 'link', 'littl', 'longev',\n",
       "        'loos', 'loud', 'low', 'mad', 'mak', 'man', 'many', 'may', 'mil',\n",
       "        'mot', 'mph', 'must', 'nev', 'new', 'newsgroup', 'nois', 'on',\n",
       "        'opin', 'overdr', 'paint', 'part', 'pickup', 'plan', 'point',\n",
       "        'post', 'problem', 'pul', 'put', 'rail', 'ratio', 'rattl', 'react',\n",
       "        'real', 'reason', 'rpm', 'rul', 'rust', 'salem', 'say', 'second',\n",
       "        'see', 'shift', 'ship', 'sign', 'situ', 'spec', 'spee', 'stabl',\n",
       "        'start', 'stay', 'steep', 'suck', 'sup', 'suppos', 'suspend',\n",
       "        'tak', 'tel', 'thing', 'third', 'three', 'tim', 'tow', 'tranny',\n",
       "        'transmit', 'truck', 'us', 'vehic', 'ventil', 'way', 'went',\n",
       "        'wrong', 'yellow'], dtype='<U39')]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter.inverse_transform(countvecs[20,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aa',\n",
       " 'aamir',\n",
       " 'aaron',\n",
       " 'ab',\n",
       " 'abandon',\n",
       " 'abbey',\n",
       " 'abbot',\n",
       " 'abbrevy',\n",
       " 'abc',\n",
       " 'abd',\n",
       " 'abdom',\n",
       " 'abdomin',\n",
       " 'abduc',\n",
       " 'abdullah',\n",
       " 'aber',\n",
       " 'aberdeen',\n",
       " 'abh',\n",
       " 'abhor',\n",
       " 'abid',\n",
       " 'abl',\n",
       " 'abnorm',\n",
       " 'aboard',\n",
       " 'abod',\n",
       " 'abol',\n",
       " 'abolit',\n",
       " 'abomin',\n",
       " 'abort',\n",
       " 'abound',\n",
       " 'abov',\n",
       " 'abraham',\n",
       " 'abram',\n",
       " 'abridg',\n",
       " 'abroad',\n",
       " 'abrog',\n",
       " 'abrupt',\n",
       " 'abs',\n",
       " 'absens',\n",
       " 'absolv',\n",
       " 'absorb',\n",
       " 'abstact',\n",
       " 'abstain',\n",
       " 'abstin',\n",
       " 'abstract',\n",
       " 'absurd',\n",
       " 'abu',\n",
       " 'abud',\n",
       " 'abund',\n",
       " 'abus',\n",
       " 'abyss',\n",
       " 'ac',\n",
       " 'acad',\n",
       " 'academ',\n",
       " 'academy',\n",
       " 'acc',\n",
       " 'acceiv',\n",
       " 'accel',\n",
       " 'acces',\n",
       " 'access',\n",
       " 'accessvisamastercard',\n",
       " 'accid',\n",
       " 'acclim',\n",
       " 'accommod',\n",
       " 'accomod',\n",
       " 'accompany',\n",
       " 'accompl',\n",
       " 'accord',\n",
       " 'account',\n",
       " 'accredit',\n",
       " 'accret',\n",
       " 'accross',\n",
       " 'accum',\n",
       " 'accus',\n",
       " 'accuss',\n",
       " 'accustom',\n",
       " 'acdc',\n",
       " 'acegr',\n",
       " 'ach',\n",
       " 'acheiv',\n",
       " 'achiev',\n",
       " 'acid',\n",
       " 'acidophil',\n",
       " 'ack',\n",
       " 'acknowledg',\n",
       " 'aclu',\n",
       " 'acm',\n",
       " 'acn',\n",
       " 'acorn',\n",
       " 'acoust',\n",
       " 'acquaint',\n",
       " 'acquir',\n",
       " 'acquisit',\n",
       " 'acquit',\n",
       " 'acr',\n",
       " 'acronym',\n",
       " 'across',\n",
       " 'act',\n",
       " 'actix',\n",
       " 'acton',\n",
       " 'actu',\n",
       " 'acupunct',\n",
       " 'acur',\n",
       " 'acut',\n",
       " 'ad',\n",
       " 'adam',\n",
       " 'adamof',\n",
       " 'adan',\n",
       " 'adapt',\n",
       " 'adaptec',\n",
       " 'adb',\n",
       " 'adc',\n",
       " 'adcom',\n",
       " 'addict',\n",
       " 'addison',\n",
       " 'addisonwesley',\n",
       " 'addit',\n",
       " 'addon',\n",
       " 'addr',\n",
       " 'address',\n",
       " 'adept',\n",
       " 'adequ',\n",
       " 'adh',\n",
       " 'adher',\n",
       " 'adirondack',\n",
       " 'adjac',\n",
       " 'adject',\n",
       " 'adjud',\n",
       " 'adjust',\n",
       " 'adl',\n",
       " 'adlib',\n",
       " 'adm',\n",
       " 'admin',\n",
       " 'adminst',\n",
       " 'admir',\n",
       " 'admiss',\n",
       " 'admit',\n",
       " 'admon',\n",
       " 'admonit',\n",
       " 'adob',\n",
       " 'adolesc',\n",
       " 'adolf',\n",
       " 'adopt',\n",
       " 'adorn',\n",
       " 'adr',\n",
       " 'adress',\n",
       " 'ads',\n",
       " 'adult',\n",
       " 'adultery',\n",
       " 'adv',\n",
       " 'advers',\n",
       " 'advert',\n",
       " 'advoc',\n",
       " 'aeg',\n",
       " 'aer',\n",
       " 'aero',\n",
       " 'aerobrak',\n",
       " 'aerodynam',\n",
       " 'aeronaut',\n",
       " 'aerospac',\n",
       " 'aerostitch',\n",
       " 'aesthet',\n",
       " 'af',\n",
       " 'aff',\n",
       " 'affair',\n",
       " 'affect',\n",
       " 'affin',\n",
       " 'affirm',\n",
       " 'afflict',\n",
       " 'afford',\n",
       " 'affy',\n",
       " 'afgh',\n",
       " 'afr',\n",
       " 'afraid',\n",
       " 'afric',\n",
       " 'africanam',\n",
       " 'aft',\n",
       " 'afteral',\n",
       " 'afterl',\n",
       " 'afterma',\n",
       " 'aftermarket',\n",
       " 'afternoon',\n",
       " 'afterthought',\n",
       " 'afterward',\n",
       " 'ag',\n",
       " 'again',\n",
       " 'agdam',\n",
       " 'agend',\n",
       " 'agenda',\n",
       " 'aggrav',\n",
       " 'aggreg',\n",
       " 'aggres',\n",
       " 'aggress',\n",
       " 'agiss',\n",
       " 'agnost',\n",
       " 'ago',\n",
       " 'agony',\n",
       " 'agr',\n",
       " 'agree',\n",
       " 'agress',\n",
       " 'agricult',\n",
       " 'ah',\n",
       " 'ahalinin',\n",
       " 'aharon',\n",
       " 'ahead',\n",
       " 'ahem',\n",
       " 'ahh',\n",
       " 'ahhh',\n",
       " 'ahl',\n",
       " 'ahm',\n",
       " 'ahmad',\n",
       " 'ahmet',\n",
       " 'ahol',\n",
       " 'ahold',\n",
       " 'ahv',\n",
       " 'ai',\n",
       " 'aia',\n",
       " 'aid',\n",
       " 'aik',\n",
       " 'ail',\n",
       " 'aim',\n",
       " 'aip',\n",
       " 'air',\n",
       " 'airb',\n",
       " 'airborn',\n",
       " 'aircraft',\n",
       " 'airf',\n",
       " 'airlin',\n",
       " 'airmail',\n",
       " 'airpl',\n",
       " 'airport',\n",
       " 'airwav',\n",
       " 'airway',\n",
       " 'aix',\n",
       " 'aj',\n",
       " 'ak',\n",
       " 'akarl',\n",
       " 'akboy',\n",
       " 'akgun',\n",
       " 'akhalkalak',\n",
       " 'akin',\n",
       " 'aksin',\n",
       " 'al',\n",
       " 'ala',\n",
       " 'alabam',\n",
       " 'alain',\n",
       " 'alamo',\n",
       " 'alarm',\n",
       " 'alask',\n",
       " 'alb',\n",
       " 'alban',\n",
       " 'albany',\n",
       " 'albedo',\n",
       " 'albeit',\n",
       " 'albert',\n",
       " 'albuquerqu',\n",
       " 'alchemy',\n",
       " 'alcohol',\n",
       " 'ald',\n",
       " 'alert',\n",
       " 'alex',\n",
       " 'alexand',\n",
       " 'alexandr',\n",
       " 'alexe',\n",
       " 'alf',\n",
       " 'alfr',\n",
       " 'algebr',\n",
       " 'algebra',\n",
       " 'alger',\n",
       " 'algorithm',\n",
       " 'algorythm',\n",
       " 'alia',\n",
       " 'alias',\n",
       " 'alice',\n",
       " 'align',\n",
       " 'alik',\n",
       " 'aliy',\n",
       " 'allah',\n",
       " 'allaround',\n",
       " 'alleg',\n",
       " 'allegy',\n",
       " 'allerg',\n",
       " 'allergy',\n",
       " 'allevy',\n",
       " 'alley',\n",
       " 'allknow',\n",
       " 'alloc',\n",
       " 'allot',\n",
       " 'allout',\n",
       " 'allow',\n",
       " 'alloy',\n",
       " 'allready',\n",
       " 'allst',\n",
       " 'alltim',\n",
       " 'allud',\n",
       " 'almanac',\n",
       " 'almighty',\n",
       " 'almost',\n",
       " 'alom',\n",
       " 'alon',\n",
       " 'along',\n",
       " 'alongsid',\n",
       " 'alot',\n",
       " 'aloud',\n",
       " 'alp',\n",
       " 'alph',\n",
       " 'alphabet',\n",
       " 'alpin',\n",
       " 'already',\n",
       " 'alright',\n",
       " 'also',\n",
       " 'alt',\n",
       " 'altath',\n",
       " 'altatheismarchivenam',\n",
       " 'altatheismmod',\n",
       " 'altbinariespict',\n",
       " 'altbinariespicturesutil',\n",
       " 'altconspir',\n",
       " 'altern',\n",
       " 'although',\n",
       " 'altim',\n",
       " 'altimet',\n",
       " 'altinay',\n",
       " 'altitud',\n",
       " 'alto',\n",
       " 'altogeth',\n",
       " 'altprivacyclip',\n",
       " 'altru',\n",
       " 'altsecuritypgp',\n",
       " 'altsecurityripem',\n",
       " 'altsex',\n",
       " 'altsourc',\n",
       " 'alumin',\n",
       " 'alumn',\n",
       " 'alway',\n",
       " 'aly',\n",
       " 'am',\n",
       " 'amaz',\n",
       " 'amb',\n",
       " 'ambassad',\n",
       " 'ambigu',\n",
       " 'ambit',\n",
       " 'ambros',\n",
       " 'ambush',\n",
       " 'amby',\n",
       " 'amd',\n",
       " 'amend',\n",
       " 'americ',\n",
       " 'america',\n",
       " 'amesarcnasagov',\n",
       " 'amfm',\n",
       " 'amherst',\n",
       " 'amid',\n",
       " 'amig',\n",
       " 'amiga',\n",
       " 'amigado',\n",
       " 'amigaphysikunizhch',\n",
       " 'amin',\n",
       " 'aminet',\n",
       " 'amino',\n",
       " 'ammend',\n",
       " 'ammo',\n",
       " 'ammon',\n",
       " 'ammunit',\n",
       " 'amnesty',\n",
       " 'amo',\n",
       " 'amok',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'amont',\n",
       " 'amorc',\n",
       " 'amount',\n",
       " 'amp',\n",
       " 'ampl',\n",
       " 'amplitud',\n",
       " 'amput',\n",
       " 'ams',\n",
       " 'amsterdam',\n",
       " 'amt',\n",
       " 'amus',\n",
       " 'amy',\n",
       " 'an',\n",
       " 'ana',\n",
       " 'anachron',\n",
       " 'anadolu',\n",
       " 'anadolunun',\n",
       " 'anaheim',\n",
       " 'analges',\n",
       " 'analog',\n",
       " 'analys',\n",
       " 'analyst',\n",
       " 'anarch',\n",
       " 'anarchy',\n",
       " 'anasin',\n",
       " 'anasir',\n",
       " 'anatol',\n",
       " 'anatom',\n",
       " 'ancest',\n",
       " 'ancestry',\n",
       " 'anch',\n",
       " 'and',\n",
       " 'anderson',\n",
       " 'andersson',\n",
       " 'andlasmas',\n",
       " 'andr',\n",
       " 'andre',\n",
       " 'andrea',\n",
       " 'andres',\n",
       " 'andrew',\n",
       " 'andreychuk',\n",
       " 'andy',\n",
       " 'anecdot',\n",
       " 'anem',\n",
       " 'anesthes',\n",
       " 'ang',\n",
       " 'angel',\n",
       " 'angl',\n",
       " 'angry',\n",
       " 'angul',\n",
       " 'anim',\n",
       " 'ankar',\n",
       " 'ankl',\n",
       " 'ann',\n",
       " 'annex',\n",
       " 'annihil',\n",
       " 'annivers',\n",
       " 'annot',\n",
       " 'annount',\n",
       " 'annoy',\n",
       " 'annul',\n",
       " 'anoint',\n",
       " 'anoma',\n",
       " 'anon',\n",
       " 'anonym',\n",
       " 'anoth',\n",
       " 'ans',\n",
       " 'anselm',\n",
       " 'answ',\n",
       " 'ant',\n",
       " 'antagon',\n",
       " 'antarct',\n",
       " 'antenn',\n",
       " 'antenna',\n",
       " 'anthem',\n",
       " 'anthony',\n",
       " 'anthropolog',\n",
       " 'antiabort',\n",
       " 'antiaircraft',\n",
       " 'antialias',\n",
       " 'antiarm',\n",
       " 'antibiot',\n",
       " 'antibody',\n",
       " 'antichr',\n",
       " 'anticip',\n",
       " 'antidefam',\n",
       " 'antidepress',\n",
       " 'antidiscrimin',\n",
       " 'antifung',\n",
       " 'antig',\n",
       " 'antigay',\n",
       " 'antigun',\n",
       " 'antih',\n",
       " 'antihistamin',\n",
       " 'antiisrael',\n",
       " 'antijew',\n",
       " 'antimat',\n",
       " 'antimuslim',\n",
       " 'antioch',\n",
       " 'antisemit',\n",
       " 'antisoc',\n",
       " 'antitank',\n",
       " 'antitrust',\n",
       " 'antivir',\n",
       " 'anton',\n",
       " 'antonio',\n",
       " 'antranik',\n",
       " 'anxy',\n",
       " 'anybody',\n",
       " 'anyhow',\n",
       " 'anym',\n",
       " 'anyon',\n",
       " 'anyth',\n",
       " 'anytim',\n",
       " 'anyway',\n",
       " 'anywh',\n",
       " 'ap',\n",
       " 'apart',\n",
       " 'apd',\n",
       " 'apert',\n",
       " 'apex',\n",
       " 'aphel',\n",
       " 'apiec',\n",
       " 'apocalyps',\n",
       " 'apocalypt',\n",
       " 'apocryph',\n",
       " 'apollo',\n",
       " 'apolog',\n",
       " 'apologet',\n",
       " 'apost',\n",
       " 'apostasy',\n",
       " 'apostl',\n",
       " 'apostol',\n",
       " 'app',\n",
       " 'appal',\n",
       " 'apparat',\n",
       " 'apparit',\n",
       " 'appdefault',\n",
       " 'appear',\n",
       " 'appeas',\n",
       " 'appel',\n",
       " 'append',\n",
       " 'appendix',\n",
       " 'appetit',\n",
       " 'appl',\n",
       " 'applaud',\n",
       " 'applaus',\n",
       " 'applelink',\n",
       " 'appletalk',\n",
       " 'applicationshellwidgetclass',\n",
       " 'apply',\n",
       " 'appoint',\n",
       " 'appolog',\n",
       " 'appr',\n",
       " 'appra',\n",
       " 'apprecy',\n",
       " 'apprehend',\n",
       " 'appress',\n",
       " 'appricy',\n",
       " 'approach',\n",
       " 'appropry',\n",
       " 'approv',\n",
       " 'approx',\n",
       " 'approxim',\n",
       " 'apr',\n",
       " 'april',\n",
       " 'apt',\n",
       " 'aqu',\n",
       " 'aquina',\n",
       " 'aquir',\n",
       " 'ar',\n",
       " 'arab',\n",
       " 'arabam',\n",
       " 'araf',\n",
       " 'arb',\n",
       " 'arbit',\n",
       " 'arc',\n",
       " 'arcad',\n",
       " 'arch',\n",
       " 'archa',\n",
       " 'archaeolog',\n",
       " 'archangel',\n",
       " 'archbishop',\n",
       " 'archeolog',\n",
       " 'archim',\n",
       " 'architect',\n",
       " 'archiveafitafmil',\n",
       " 'archivenam',\n",
       " 'archiveserv',\n",
       " 'archy',\n",
       " 'arcinfo',\n",
       " 'arct',\n",
       " 'ard',\n",
       " 'are',\n",
       " 'area',\n",
       " 'areil',\n",
       " 'aren',\n",
       " 'arena',\n",
       " 'arf',\n",
       " 'arg',\n",
       " 'argc',\n",
       " 'argentin',\n",
       " 'argh',\n",
       " 'argu',\n",
       " 'argv',\n",
       " 'ariel',\n",
       " 'aristid',\n",
       " 'aristotl',\n",
       " 'arithmet',\n",
       " 'arizon',\n",
       " 'ark',\n",
       " 'arkansa',\n",
       " 'arl',\n",
       " 'arlington',\n",
       " 'arm',\n",
       " 'arma',\n",
       " 'armageddon',\n",
       " 'armchair',\n",
       " 'armen',\n",
       " 'armeniannaz',\n",
       " 'armeny',\n",
       " 'armo',\n",
       " 'armstrong',\n",
       " 'army',\n",
       " 'arnold',\n",
       " 'aros',\n",
       " 'around',\n",
       " 'arp',\n",
       " 'arrang',\n",
       " 'array',\n",
       " 'arrest',\n",
       " 'arrhythmia',\n",
       " 'arrl',\n",
       " 'arrog',\n",
       " 'arromd',\n",
       " 'arround',\n",
       " 'arrow',\n",
       " 'ars',\n",
       " 'arsenokoita',\n",
       " 'arsiv',\n",
       " 'art',\n",
       " 'artery',\n",
       " 'arth',\n",
       " 'arthrit',\n",
       " 'artic',\n",
       " 'artifact',\n",
       " 'artillery',\n",
       " 'artin',\n",
       " 'artwork',\n",
       " 'ary',\n",
       " 'as',\n",
       " 'asaf',\n",
       " 'asal',\n",
       " 'asalasdpaarf',\n",
       " 'asc',\n",
       " 'ascend',\n",
       " 'ascertain',\n",
       " 'asci',\n",
       " 'ascrib',\n",
       " 'ash',\n",
       " 'asham',\n",
       " 'ashby',\n",
       " 'ashc',\n",
       " 'ashok',\n",
       " 'ashton',\n",
       " 'ashtray',\n",
       " 'asid',\n",
       " 'asimov',\n",
       " 'ask',\n",
       " 'asker',\n",
       " 'askew',\n",
       " 'asleep',\n",
       " 'asm',\n",
       " 'asp',\n",
       " 'aspect',\n",
       " 'asphalt',\n",
       " 'aspir',\n",
       " 'aspirin',\n",
       " 'ass',\n",
       " 'assad',\n",
       " 'assail',\n",
       " 'assasin',\n",
       " 'assassin',\n",
       " 'assault',\n",
       " 'assay',\n",
       " 'assembl',\n",
       " 'assert',\n",
       " 'assess',\n",
       " 'asset',\n",
       " 'asshol',\n",
       " 'assign',\n",
       " 'assimil',\n",
       " 'assist',\n",
       " 'assoc',\n",
       " 'assocy',\n",
       " 'assort',\n",
       " 'assualt',\n",
       " 'assult',\n",
       " 'assum',\n",
       " 'assyr',\n",
       " 'ast',\n",
       " 'asterixinescnpt',\n",
       " 'asteroid',\n",
       " 'asthm',\n",
       " 'aston',\n",
       " 'astound',\n",
       " 'astr',\n",
       " 'astray',\n",
       " 'astro',\n",
       " 'astrolog',\n",
       " 'astronaut',\n",
       " 'astronom',\n",
       " 'astrophys',\n",
       " 'asum',\n",
       " 'asw',\n",
       " 'asy',\n",
       " 'asymmet',\n",
       " 'asymptom',\n",
       " 'asynchron',\n",
       " 'at',\n",
       " 'atam',\n",
       " 'atar',\n",
       " 'atas',\n",
       " 'ataturk',\n",
       " 'atb',\n",
       " 'atf',\n",
       " 'atfedil',\n",
       " 'atffb',\n",
       " 'ath',\n",
       " 'athen',\n",
       " 'athiest',\n",
       " 'athlet',\n",
       " 'atkin',\n",
       " 'atl',\n",
       " 'atla',\n",
       " 'atlant',\n",
       " 'atleast',\n",
       " 'atm',\n",
       " 'atmosph',\n",
       " 'atom',\n",
       " 'aton',\n",
       " 'atop',\n",
       " 'atroc',\n",
       " 'atroph',\n",
       " 'attach',\n",
       " 'attack',\n",
       " 'attain',\n",
       " 'attempt',\n",
       " 'attend',\n",
       " 'attest',\n",
       " 'attitud',\n",
       " 'attn',\n",
       " 'attorney',\n",
       " 'attract',\n",
       " 'attribut',\n",
       " 'au',\n",
       " 'auct',\n",
       " 'aud',\n",
       " 'audet',\n",
       " 'audio',\n",
       " 'audiophil',\n",
       " 'audiovideo',\n",
       " 'audit',\n",
       " 'auditor',\n",
       " 'audy',\n",
       " 'aug',\n",
       " 'august',\n",
       " 'augustin',\n",
       " 'aunt',\n",
       " 'aur',\n",
       " 'auschwitz',\n",
       " 'aust',\n",
       " 'austin',\n",
       " 'austr',\n",
       " 'austral',\n",
       " 'auth',\n",
       " 'authorit',\n",
       " 'auto',\n",
       " 'autobahn',\n",
       " 'autobiograph',\n",
       " 'autocad',\n",
       " 'autodesk',\n",
       " 'autoexec',\n",
       " 'autoexecb',\n",
       " 'autofoc',\n",
       " 'autograph',\n",
       " 'autom',\n",
       " 'automobl',\n",
       " 'automot',\n",
       " 'autonom',\n",
       " 'autopsy',\n",
       " 'autotrac',\n",
       " 'autoweek',\n",
       " 'autumn',\n",
       " 'aux',\n",
       " 'auxy',\n",
       " 'av',\n",
       " 'avail',\n",
       " 'avalon',\n",
       " 'avaly',\n",
       " 'aveng',\n",
       " 'avenu',\n",
       " 'avert',\n",
       " 'avet',\n",
       " 'avg',\n",
       " 'avid',\n",
       " 'avigd',\n",
       " 'avoid',\n",
       " 'avow',\n",
       " 'avy',\n",
       " 'aw',\n",
       " 'await',\n",
       " 'awak',\n",
       " 'award',\n",
       " 'away',\n",
       " 'awd',\n",
       " 'awesom',\n",
       " 'awhil',\n",
       " 'awkward',\n",
       " 'ax',\n",
       " 'axax',\n",
       " 'axel',\n",
       " 'axelsson',\n",
       " 'axiom',\n",
       " 'axl',\n",
       " 'axp',\n",
       " 'ay',\n",
       " 'ayn',\n",
       " 'az',\n",
       " 'azer',\n",
       " 'azerbads',\n",
       " 'azerbaid',\n",
       " 'azerbaijan',\n",
       " 'azrael',\n",
       " 'baalk',\n",
       " 'bab',\n",
       " 'babbl',\n",
       " 'babial',\n",
       " 'baby',\n",
       " 'babych',\n",
       " 'babylon',\n",
       " 'bach',\n",
       " 'bachel',\n",
       " 'back',\n",
       " 'backbon',\n",
       " 'backdo',\n",
       " 'backdraft',\n",
       " 'backdrop',\n",
       " 'backfir',\n",
       " 'background',\n",
       " 'backhand',\n",
       " 'backlit',\n",
       " 'backord',\n",
       " 'backpack',\n",
       " 'backrest',\n",
       " 'backstab',\n",
       " 'backup',\n",
       " 'backward',\n",
       " 'backyard',\n",
       " 'bact',\n",
       " 'bacter',\n",
       " 'bad',\n",
       " 'badg',\n",
       " 'badmatch',\n",
       " 'baerg',\n",
       " 'baffl',\n",
       " 'bag',\n",
       " 'baghdad',\n",
       " 'bahama',\n",
       " 'baik',\n",
       " 'bail',\n",
       " 'bailey',\n",
       " 'bain',\n",
       " 'bait',\n",
       " 'bak',\n",
       " 'bakanlig',\n",
       " 'baku',\n",
       " 'bal',\n",
       " 'balcony',\n",
       " 'bald',\n",
       " 'balk',\n",
       " 'ballard',\n",
       " 'ballast',\n",
       " 'ballgam',\n",
       " 'balloon',\n",
       " 'ballot',\n",
       " 'ballpark',\n",
       " 'ballplay',\n",
       " 'ballyard',\n",
       " 'baltim',\n",
       " 'bamford',\n",
       " 'ban',\n",
       " 'band',\n",
       " 'bandit',\n",
       " 'bandwagon',\n",
       " 'bandwi',\n",
       " 'bandwid',\n",
       " 'bandy',\n",
       " 'bang',\n",
       " 'bank',\n",
       " 'banko',\n",
       " 'bankrupt',\n",
       " 'bankruptcy',\n",
       " 'banschbach',\n",
       " 'bantam',\n",
       " 'bapt',\n",
       " 'bar',\n",
       " 'barb',\n",
       " 'barbar',\n",
       " 'barbecu',\n",
       " 'barehand',\n",
       " 'barf',\n",
       " 'bargain',\n",
       " 'bark',\n",
       " 'barn',\n",
       " 'barney',\n",
       " 'baron',\n",
       " 'barrack',\n",
       " 'barrasso',\n",
       " 'barrel',\n",
       " 'barret',\n",
       " 'barry',\n",
       " 'bart',\n",
       " 'bartholomew',\n",
       " 'barton',\n",
       " 'bas',\n",
       " 'basbakanlik',\n",
       " 'basebal',\n",
       " 'baseless',\n",
       " 'baselin',\n",
       " 'basem',\n",
       " 'baserun',\n",
       " 'basest',\n",
       " 'baset',\n",
       " 'bash',\n",
       " 'basil',\n",
       " 'basim',\n",
       " 'basin',\n",
       " 'bask',\n",
       " 'basket',\n",
       " 'basketbal',\n",
       " 'baslangicind',\n",
       " 'bass',\n",
       " 'bastard',\n",
       " 'bat',\n",
       " 'batch',\n",
       " 'batf',\n",
       " 'batffb',\n",
       " 'bath',\n",
       " 'bathroom',\n",
       " 'batm',\n",
       " 'bats',\n",
       " 'battery',\n",
       " 'battl',\n",
       " 'battlefield',\n",
       " 'bau',\n",
       " 'baud',\n",
       " 'baumgartn',\n",
       " 'bautin',\n",
       " 'bay',\n",
       " 'bayl',\n",
       " 'bayonet',\n",
       " 'baz',\n",
       " 'baza',\n",
       " 'bbc',\n",
       " 'bboard',\n",
       " 'bbs',\n",
       " 'bbses',\n",
       " 'bbss',\n",
       " 'bcci',\n",
       " 'bce',\n",
       " 'bchmbiochemdukeedu',\n",
       " 'bcop',\n",
       " 'bdf',\n",
       " 'bdi',\n",
       " 'bds',\n",
       " 'beach',\n",
       " 'beacon',\n",
       " 'bead',\n",
       " 'beal',\n",
       " 'beam',\n",
       " 'bean',\n",
       " 'bear',\n",
       " 'beard',\n",
       " 'beast',\n",
       " 'beastmast',\n",
       " 'beat',\n",
       " 'beauchain',\n",
       " 'beaupr',\n",
       " 'beauregard',\n",
       " 'beauty',\n",
       " 'beav',\n",
       " 'beaverton',\n",
       " 'becam',\n",
       " 'becasu',\n",
       " 'beck',\n",
       " 'beckm',\n",
       " 'becom',\n",
       " 'becuas',\n",
       " 'bed',\n",
       " 'bedford',\n",
       " 'bedouin',\n",
       " 'bedroom',\n",
       " 'bee',\n",
       " 'beef',\n",
       " 'beem',\n",
       " 'beep',\n",
       " 'beer',\n",
       " 'beet',\n",
       " 'beez',\n",
       " 'befal',\n",
       " 'befel',\n",
       " 'beforehand',\n",
       " 'beg',\n",
       " 'beget',\n",
       " 'begg',\n",
       " 'begin',\n",
       " 'begot',\n",
       " 'begun',\n",
       " 'behalf',\n",
       " 'behav',\n",
       " 'behavio',\n",
       " 'behavy',\n",
       " 'behind',\n",
       " 'behold',\n",
       " 'being',\n",
       " 'beirut',\n",
       " 'bel',\n",
       " 'belab',\n",
       " 'belch',\n",
       " 'beleiv',\n",
       " 'belfo',\n",
       " 'belg',\n",
       " ...]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TfidfVectorizer :)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(tokenizer=preprocessor,min_df=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(corpus.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12419"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  11.,   56.,  183.,  359.,  511.,  743., 1000., 1546., 2336.,\n",
       "        5674.]),\n",
       " array([2.09834703, 2.79374728, 3.48914754, 4.18454779, 4.87994804,\n",
       "        5.57534829, 6.27074855, 6.9661488 , 7.66154905, 8.35694931,\n",
       "        9.05234956]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAEMNJREFUeJzt3XmMXeV5x/HvL5gskAUCBlHb1FSxopBKAWQBLRJqcWqWRIFWQXLUJhZCclXRirSVUsg/KAtSkKoQRWqQLKBx0jSOC0GgBIVYLG3zB4tZwuYgO4SAY4qdGkgozWLy9I/7Oh1gljv2zNwZv9+PNLrnPOe99zzH8sxvznvPuZOqQpLUnzeMugFJ0mgYAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROLRp1A5M5+uija/ny5aNuQ5IWlPvvv/+nVbV4qnHzOgCWL1/Oli1bRt2GJC0oSX48zDingCSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVPz+k5gSRql5Zd9e2T7fupzH5j1fXgGIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkTg0VAEmeSvJIkoeSbGm1dybZnGRbezyy1ZPki0m2J3k4ySljXmdtG78tydrZOSRJ0jCmcwbwx1V1UlWtbOuXAbdX1Qrg9rYOcC6won2tA66BQWAAVwCnAacCV+wLDUnS3DuQKaDzgQ1teQNwwZj6V2rgbuCIJMcBZwObq2pPVT0PbAbOOYD9S5IOwLABUMB3k9yfZF2rHVtVzwK0x2NafQnwzJjn7mi1ieqvkmRdki1JtuzevXv4I5EkTcuiIcedUVU7kxwDbE7yg0nGZpxaTVJ/daFqPbAeYOXKla/bLkmaGUOdAVTVzva4C7iJwRz+c21qh/a4qw3fASwb8/SlwM5J6pKkEZgyAJIcnuRt+5aB1cCjwC3Avit51gI3t+VbgI+1q4FOB15sU0S3AauTHNne/F3dapKkERhmCuhY4KYk+8b/a1V9J8l9wKYkFwNPAxe28bcC5wHbgZeBiwCqak+SzwD3tXGfrqo9M3YkkqRpmTIAqupJ4H3j1P8bWDVOvYBLJnit64Hrp9+mJGmmeSewJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktSpoQMgySFJHkzyrbZ+QpJ7kmxL8o0kb2z1N7X17W378jGvcXmrP5Hk7Jk+GEnS8KZzBnApsHXM+lXA1VW1AngeuLjVLwaer6p3AVe3cSQ5EVgDvBc4B/hSkkMOrH1J0v4aKgCSLAU+AFzb1gOcBdzQhmwALmjL57d12vZVbfz5wMaq+mVV/QjYDpw6EwchSZq+Yc8AvgB8AvhNWz8KeKGq9rb1HcCStrwEeAagbX+xjf9tfZznSJLm2JQBkOSDwK6qun9seZyhNcW2yZ4zdn/rkmxJsmX37t1TtSdJ2k/DnAGcAXwoyVPARgZTP18AjkiyqI1ZCuxsyzuAZQBt+zuAPWPr4zznt6pqfVWtrKqVixcvnvYBSZKGM2UAVNXlVbW0qpYzeBP3jqr6c+BO4MNt2Frg5rZ8S1unbb+jqqrV17SrhE4AVgD3ztiRSJKmZdHUQyb0D8DGJJ8FHgSua/XrgK8m2c7gN/81AFX1WJJNwOPAXuCSqnrlAPYvSToA0wqAqroLuKstP8k4V/FU1S+ACyd4/pXAldNtUpI087wTWJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ2aMgCSvDnJvUm+n+SxJJ9q9ROS3JNkW5JvJHljq7+prW9v25ePea3LW/2JJGfP1kFJkqY2zBnAL4Gzqup9wEnAOUlOB64Crq6qFcDzwMVt/MXA81X1LuDqNo4kJwJrgPcC5wBfSnLITB6MJGl4UwZADbzUVg9tXwWcBdzQ6huAC9ry+W2dtn1VkrT6xqr6ZVX9CNgOnDojRyFJmrah3gNIckiSh4BdwGbgh8ALVbW3DdkBLGnLS4BnANr2F4GjxtbHec7Yfa1LsiXJlt27d0//iCRJQxkqAKrqlao6CVjK4Lf294w3rD1mgm0T1V+7r/VVtbKqVi5evHiY9iRJ+2FaVwFV1QvAXcDpwBFJFrVNS4GdbXkHsAygbX8HsGdsfZznSJLm2DBXAS1OckRbfgvwfmArcCfw4TZsLXBzW76lrdO231FV1epr2lVCJwArgHtn6kAkSdOzaOohHAdsaFfsvAHYVFXfSvI4sDHJZ4EHgeva+OuArybZzuA3/zUAVfVYkk3A48Be4JKqemVmD0eSNKwpA6CqHgZOHqf+JONcxVNVvwAunOC1rgSunH6bkqSZ5p3AktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqeG+aPwkjRSyy/79qhbOCh5BiBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqemDIAky5LcmWRrkseSXNrq70yyOcm29nhkqyfJF5NsT/JwklPGvNbaNn5bkrWzd1iSpKkMcwawF/j7qnoPcDpwSZITgcuA26tqBXB7Wwc4F1jRvtYB18AgMIArgNOAU4Er9oWGJGnuTRkAVfVsVT3Qln8ObAWWAOcDG9qwDcAFbfl84Cs1cDdwRJLjgLOBzVW1p6qeBzYD58zo0UiShjat9wCSLAdOBu4Bjq2qZ2EQEsAxbdgS4JkxT9vRahPVJUkjMHQAJHkrcCPw8ar62WRDx6nVJPXX7mddki1JtuzevXvY9iRJ0zRUACQ5lMEP/69V1Tdb+bk2tUN73NXqO4BlY56+FNg5Sf1Vqmp9Va2sqpWLFy+ezrFIkqZhmKuAAlwHbK2qz4/ZdAuw70qetcDNY+ofa1cDnQ682KaIbgNWJzmyvfm7utUkSSMwzMdBnwF8FHgkyUOt9kngc8CmJBcDTwMXtm23AucB24GXgYsAqmpPks8A97Vxn66qPTNyFJKkaZsyAKrqe4w/fw+wapzxBVwywWtdD1w/nQYlSbPDO4ElqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKRODfNhcJIEwPLLvj3qFjSDPAOQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHVqygBIcn2SXUkeHVN7Z5LNSba1xyNbPUm+mGR7koeTnDLmOWvb+G1J1s7O4UiShjXMGcCXgXNeU7sMuL2qVgC3t3WAc4EV7WsdcA0MAgO4AjgNOBW4Yl9oSJJGY8oAqKr/APa8pnw+sKEtbwAuGFP/Sg3cDRyR5DjgbGBzVe2pqueBzbw+VCRJc2h//ybwsVX1LEBVPZvkmFZfAjwzZtyOVpuo/jpJ1jE4e+D444/fz/akg5d/l1czZabfBM44tZqk/vpi1fqqWllVKxcvXjyjzUmS/t/+BsBzbWqH9rir1XcAy8aMWwrsnKQuSRqR/Q2AW4B9V/KsBW4eU/9YuxrodODFNlV0G7A6yZHtzd/VrSZJGpEp3wNI8nXgj4Cjk+xgcDXP54BNSS4GngYubMNvBc4DtgMvAxcBVNWeJJ8B7mvjPl1Vr31jWZI0h6YMgKr6yASbVo0ztoBLJnid64Hrp9WdJGnWeCewJHXKAJCkThkAktQpA0CSOrW/dwJL3fOOXC10ngFIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQp7wPQgua1+NL+8wxAkjplAEhSpwwASeqUASBJnTIAJKlTXgWkGeHVONLC4xmAJHXKAJCkThkAktQpA0CSOmUASFKnvAroIOKVOJKmwzMASeqUASBJnZrzAEhyTpInkmxPctlc71+SNDCnAZDkEOCfgHOBE4GPJDlxLnuQJA3M9ZvApwLbq+pJgCQbgfOBx+e4j1nlm7GSFoK5DoAlwDNj1ncAp83WzvxBLEkTm+sAyDi1etWAZB2wrq2+lOSJIV/7aOCnB9DbXLPf2bWQ+l1IvYL9zrajgZ/mqgN6jd8dZtBcB8AOYNmY9aXAzrEDqmo9sH66L5xkS1WtPLD25o79zq6F1O9C6hXsd7bNZb9zfRXQfcCKJCckeSOwBrhljnuQJDHHZwBVtTfJXwO3AYcA11fVY3PZgyRpYM4/CqKqbgVunYWXnva00YjZ7+xaSP0upF7BfmfbnPWbqpp6lCTpoONHQUhSpxZ8ACRZluTOJFuTPJbk0lH3NJkkb05yb5Lvt34/NeqeppLkkCQPJvnWqHuZSpKnkjyS5KEkW0bdz1SSHJHkhiQ/aP+H/2DUPU0kybvbv+u+r58l+fio+5pIkr9t32OPJvl6kjePuqfJJLm09frYXP27LvgpoCTHAcdV1QNJ3gbcD1xQVfPy7uIkAQ6vqpeSHAp8D7i0qu4ecWsTSvJ3wErg7VX1wVH3M5kkTwErq2pBXPedZAPwn1V1bbsy7rCqemHUfU2lfazLT4DTqurHo+7ntZIsYfC9dWJV/W+STcCtVfXl0XY2viS/D2xk8GkJvwK+A/xVVW2bzf0u+DOAqnq2qh5oyz8HtjK443heqoGX2uqh7WvepnCSpcAHgGtH3cvBJsnbgTOB6wCq6lcL4Yd/swr44Xz84T/GIuAtSRYBh/Gae47mmfcAd1fVy1W1F/h34E9ne6cLPgDGSrIcOBm4Z7SdTK5NqTwE7AI2V9V87vcLwCeA34y6kSEV8N0k97e7yuez3wN2A//cptiuTXL4qJsa0hrg66NuYiJV9RPgH4GngWeBF6vqu6PtalKPAmcmOSrJYcB5vPqm2Vlx0ARAkrcCNwIfr6qfjbqfyVTVK1V1EoM7oU9tp3/zTpIPAruq6v5R9zINZ1TVKQw+cfaSJGeOuqFJLAJOAa6pqpOB/wHm/Uekt6mqDwH/NupeJpLkSAYfNHkC8DvA4Un+YrRdTayqtgJXAZsZTP98H9g72/s9KAKgzaXfCHytqr456n6G1U737wLOGXErEzkD+FCbV98InJXkX0bb0uSqamd73AXcxGBOdb7aAewYcwZ4A4NAmO/OBR6oqudG3cgk3g/8qKp2V9WvgW8CfzjiniZVVddV1SlVdSawB5jV+X84CAKgval6HbC1qj4/6n6mkmRxkiPa8lsY/Ef9wWi7Gl9VXV5VS6tqOYNT/juqat7+FpXk8HYhAG0qZTWDU+t5qar+C3gmybtbaRUL46PRP8I8nv5pngZOT3JY+xmxisH7g/NWkmPa4/HAnzEH/8YHwx+FPwP4KPBIm1cH+GS743g+Og7Y0K6ieAOwqarm/eWVC8SxwE2D73cWAf9aVd8ZbUtT+hvga21a5UngohH3M6k2P/0nwF+OupfJVNU9SW4AHmAwlfIg8/+O4BuTHAX8Grikqp6f7R0u+MtAJUn7Z8FPAUmS9o8BIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSp/4P/bd91ZXyOkMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(vectorizer.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_best = np.argsort(vectorizer.idf_)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idfs_vocabulary = vectorizer.get_feature_names()\n",
    "top_features = [tf_idfs_vocabulary[i] for i in idx_best[:top_n]]\n",
    "filtered_features = [tf_idfs_vocabulary[i] for i,idf_score in enumerate(vectorizer.idf_ > 7.5) if idf_score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8419"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5151\n"
     ]
    }
   ],
   "source": [
    "for i,t in enumerate(corpus.data):\n",
    "    if \"abcdefg\" in t:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model and Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the tf_idf model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model to the disk\n",
    "filename = 'tf_idf.sav'\n",
    "pickle.dump(vectorizer, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the tf_idf model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "vectorizer = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the BoW model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model to the disk\n",
    "filename = 'BoW.sav'\n",
    "pickle.dump(counter, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the BoW model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "counter = pickle.load(open(filename, 'rb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
